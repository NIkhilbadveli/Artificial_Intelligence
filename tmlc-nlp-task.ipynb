{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!wget https://raw.githubusercontent.com/JoseCaliz/dotfiles/main/css/custom_css.css 2>/dev/null 1>&2\n    \nfrom IPython.core.display import HTML\nwith open('./custom_css.css', 'r') as file:\n    custom_css = file.read()\n\nHTML(custom_css)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-09-18T19:51:27.348833Z","iopub.execute_input":"2022-09-18T19:51:27.349218Z","iopub.status.idle":"2022-09-18T19:51:28.567989Z","shell.execute_reply.started":"2022-09-18T19:51:27.349180Z","shell.execute_reply":"2022-09-18T19:51:28.566900Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cmap\nimport matplotlib.colors as mpl_colors\n\ndef hex_to_rgb(h):\n    h = h.lstrip('#')\n    return tuple(int(h[i:i+2], 16)/255 for i in (0, 2, 4))\n\ncluster_colors_hex = ['#b4d2b1', '#568f8b', '#1d4a60', '#cd7e59', '#ddb247', '#d15252']\ncluster_colors_rgb = [hex_to_rgb(x) for x in cluster_colors_hex]\ncmap = mpl_colors.ListedColormap(cluster_colors_rgb)\ncolors = cmap.colors\nbg_color= '#fdfcf6'\n\ncustom_params = {\n    \"axes.spines.right\": False,\n    \"axes.spines.top\": False,\n    'grid.alpha':0.3,\n    'figure.figsize': (16, 6),\n    'axes.titlesize': 'Large',\n    'axes.labelsize': 'Large',\n    'figure.facecolor': bg_color,\n    'axes.facecolor': bg_color\n}\n\nsns.set_theme(\n    style='whitegrid',\n    palette=sns.color_palette(cluster_colors_hex),\n    rc=custom_params\n)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-09-18T19:51:28.569861Z","iopub.execute_input":"2022-09-18T19:51:28.570534Z","iopub.status.idle":"2022-09-18T19:51:28.580210Z","shell.execute_reply.started":"2022-09-18T19:51:28.570495Z","shell.execute_reply":"2022-09-18T19:51:28.579335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","scrolled":true,"execution":{"iopub.status.busy":"2022-09-18T19:51:28.581475Z","iopub.execute_input":"2022-09-18T19:51:28.581984Z","iopub.status.idle":"2022-09-18T19:51:28.596291Z","shell.execute_reply.started":"2022-09-18T19:51:28.581952Z","shell.execute_reply":"2022-09-18T19:51:28.594951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# E-commerce text classification\n\n<img src='https://storage.googleapis.com/kaggle-datasets-images/2093102/3477380/44b0c6a2ff03a89726b1baa56fa36af6/dataset-cover.jpg?t=2022-04-17-12-35-05' style='width:50%; margin-left:auto; margin-right:auto'/>\n\n<span id='toc'/>\n\nThis style is taken from Jose Caliz's notebook from [here](https://www.kaggle.com/code/jcaliz/tps-sep22-eda-baseline-you-were-looking-for).\n\n# Table of Contents\n1. [Table of Contents](#Table-of-Contents)\n1. [EDA](#eda)\n1. [Preprocessing](#prepro)\n1. [Modelling](#mdl)\n1. [Hyperparamater tuning](#hpt)","metadata":{}},{"cell_type":"markdown","source":"# EDA\n\n<span id='eda'/>","metadata":{}},{"cell_type":"code","source":"# Let's read the data\ndf_input = pd.read_csv('../input/ecommerce-text-classification/ecommerceDataset.csv', header=None, names=['Target', 'Text'])\ndf_input.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-18T19:51:28.597804Z","iopub.execute_input":"2022-09-18T19:51:28.598595Z","iopub.status.idle":"2022-09-18T19:51:29.028807Z","shell.execute_reply.started":"2022-09-18T19:51:28.598543Z","shell.execute_reply":"2022-09-18T19:51:29.027988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's see how many target labels are present\ndf_input['Target'].unique()","metadata":{"execution":{"iopub.status.busy":"2022-09-17T11:21:37.117661Z","iopub.execute_input":"2022-09-17T11:21:37.118925Z","iopub.status.idle":"2022-09-17T11:21:37.137213Z","shell.execute_reply.started":"2022-09-17T11:21:37.118868Z","shell.execute_reply":"2022-09-17T11:21:37.136059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for data types and nulls\ndf_input.info()","metadata":{"execution":{"iopub.status.busy":"2022-09-17T11:21:39.254233Z","iopub.execute_input":"2022-09-17T11:21:39.254656Z","iopub.status.idle":"2022-09-17T11:21:39.282974Z","shell.execute_reply.started":"2022-09-17T11:21:39.254618Z","shell.execute_reply":"2022-09-17T11:21:39.281652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Looks like there's only one null value in the 'Text' column. So, it is safe to drop it.\ndf_input.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-09-18T19:51:29.031187Z","iopub.execute_input":"2022-09-18T19:51:29.031920Z","iopub.status.idle":"2022-09-18T19:51:29.052192Z","shell.execute_reply.started":"2022-09-18T19:51:29.031882Z","shell.execute_reply":"2022-09-18T19:51:29.050645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the maximum length (in characters) of document from 'Text' column\ndf_input['Text'].str.len().max()","metadata":{"execution":{"iopub.status.busy":"2022-09-18T20:11:30.985549Z","iopub.execute_input":"2022-09-18T20:11:30.985971Z","iopub.status.idle":"2022-09-18T20:11:31.026611Z","shell.execute_reply.started":"2022-09-18T20:11:30.985936Z","shell.execute_reply":"2022-09-18T20:11:31.025183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot histogram of these document lengths\ndf_input['Text'].str.len().hist(bins=100)\nplt.title('Distribution of document lengths (in characters)')\nplt.xlabel('Number of characters')\nplt.ylabel('Number of documents')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-18T20:11:32.541409Z","iopub.execute_input":"2022-09-18T20:11:32.541842Z","iopub.status.idle":"2022-09-18T20:11:33.086956Z","shell.execute_reply.started":"2022-09-18T20:11:32.541808Z","shell.execute_reply":"2022-09-18T20:11:33.086171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check if the target labels are imbalanced.\ntarget_val_counts = df_input['Target'].value_counts()\nsns.barplot(x=target_val_counts.index, y=target_val_counts)\nplt.title('Distribution of target labels across all the documents')\nplt.xlabel('Number of characters')\nplt.ylabel('Target label')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-17T11:26:49.635905Z","iopub.execute_input":"2022-09-17T11:26:49.636356Z","iopub.status.idle":"2022-09-17T11:26:49.889339Z","shell.execute_reply.started":"2022-09-17T11:26:49.636318Z","shell.execute_reply":"2022-09-17T11:26:49.888283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the same in a pie chart\nplt.pie(target_val_counts, labels=target_val_counts.index, colors=colors, autopct='%.0f%%')\nplt.title('Distribution of target labels across all the documents')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-17T11:29:33.177366Z","iopub.execute_input":"2022-09-17T11:29:33.177850Z","iopub.status.idle":"2022-09-17T11:29:33.317103Z","shell.execute_reply.started":"2022-09-17T11:29:33.177811Z","shell.execute_reply":"2022-09-17T11:29:33.315254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks like the data is almost balanced except for the **household** which is slightly higher than the 25% needed for a balanced dataset. Ideally, we would need to oversample the minority class or undersample the majority class if we want to completely eliminate this imbalance.","metadata":{}},{"cell_type":"code","source":"# Show a wordcloud of the entire corpus\nfrom wordcloud import WordCloud, STOPWORDS\n\ndesc_cloud = WordCloud(stopwords=STOPWORDS).generate(' '.join(df_input['Text']))\nplt.imshow(desc_cloud)\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-17T11:36:56.257634Z","iopub.execute_input":"2022-09-17T11:36:56.258103Z","iopub.status.idle":"2022-09-17T11:37:18.694775Z","shell.execute_reply.started":"2022-09-17T11:36:56.258064Z","shell.execute_reply":"2022-09-17T11:37:18.693783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Although most of the data has household items description, we can't see any keywords that suggest this. We could try updating the stopwords and then re-generating this wordcloud.\n\nAlso, it is interesting to see words like \"Stainless Steel\" which might suggest the items belong to household products. And the \"India\" can also be seen which implies that the dataset must've been collected from an Indian e-commerce website.","metadata":{}},{"cell_type":"markdown","source":"# Preprocessing\n\n<span id='prepro'/>","metadata":{}},{"cell_type":"markdown","source":"## Remove punctuations","metadata":{}},{"cell_type":"code","source":"# Use Pandas apply() to remove punctuation for every document in an efficient manner\nimport string\n\ndf_input['Text'] = df_input['Text'].apply(lambda x: \n                                          x.translate(str.maketrans('', '', string.punctuation)))\n\n# Verify that it actually worked\ndf_input['Text'][0]","metadata":{"execution":{"iopub.status.busy":"2022-09-18T20:11:42.370517Z","iopub.execute_input":"2022-09-18T20:11:42.370901Z","iopub.status.idle":"2022-09-18T20:11:44.017704Z","shell.execute_reply.started":"2022-09-18T20:11:42.370871Z","shell.execute_reply":"2022-09-18T20:11:44.016518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Remove URLs (if any)\n\nLooks, there aren't any URLs (at least according the RegEx pattern used below.)","metadata":{}},{"cell_type":"code","source":"import re\n\nURL_PATTERN = r'\\s*https?://\\S+(\\s+|$)'\ndf_input['urlcount'] = df_input['Text'].apply(lambda x: re.findall(URL_PATTERN, x)).str.len()\ndf_input['urlcount'].unique()","metadata":{"execution":{"iopub.status.busy":"2022-09-17T12:27:31.009068Z","iopub.execute_input":"2022-09-17T12:27:31.009471Z","iopub.status.idle":"2022-09-17T12:27:32.141564Z","shell.execute_reply.started":"2022-09-17T12:27:31.009438Z","shell.execute_reply":"2022-09-17T12:27:32.140405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Convert to lowercase","metadata":{}},{"cell_type":"code","source":"# Use apply() function and lower() to achieve this\ndf_input['Text'] = df_input['Text'].apply(lambda x: x.lower())\n\n# Verify that it actually worked\ndf_input['Text'][0]","metadata":{"execution":{"iopub.status.busy":"2022-09-18T20:11:47.124467Z","iopub.execute_input":"2022-09-18T20:11:47.125656Z","iopub.status.idle":"2022-09-18T20:11:47.228865Z","shell.execute_reply.started":"2022-09-18T20:11:47.125605Z","shell.execute_reply":"2022-09-18T20:11:47.227506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tokenization","metadata":{}},{"cell_type":"code","source":"from nltk.tokenize import word_tokenize\n\ndf_input['tokenized_text'] = df_input['Text'].apply(lambda x: word_tokenize(x))\ndf_input['tokenized_text'].head()","metadata":{"execution":{"iopub.status.busy":"2022-09-18T20:11:49.266378Z","iopub.execute_input":"2022-09-18T20:11:49.266786Z","iopub.status.idle":"2022-09-18T20:12:14.317399Z","shell.execute_reply.started":"2022-09-18T20:11:49.266753Z","shell.execute_reply":"2022-09-18T20:12:14.315843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Remove stopwords","metadata":{}},{"cell_type":"code","source":"# Import the nlp library\nimport nltk\n\n# Stop words present in the library\nstopwords = nltk.corpus.stopwords.words('english')\n\n# Use apply() on the 'Text' column\ndf_input['tokenized_text'] = df_input['tokenized_text'].apply(lambda x: [word for word in x if word not in stopwords])\ndf_input['tokenized_text'].head()","metadata":{"execution":{"iopub.status.busy":"2022-09-18T20:12:17.759355Z","iopub.execute_input":"2022-09-18T20:12:17.761418Z","iopub.status.idle":"2022-09-18T20:12:32.113223Z","shell.execute_reply.started":"2022-09-18T20:12:17.761339Z","shell.execute_reply":"2022-09-18T20:12:32.111808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Stemming\n\nThe process of stemming converts a word to its root form. \nFor example:- \"programmer\", \"programming\" ==> \"program\"\n\nBut, it might also convert to meaningless words in some cases.\nEx:- \"Goose\" ==> \"Goos\"","metadata":{}},{"cell_type":"code","source":"# Use one of NLTK's in-built stemmers\nfrom nltk.stem import SnowballStemmer\n\nstemmer = SnowballStemmer('english')\n\n# Apply for the first example\nprint([stemmer.stem(x) for x in df_input['tokenized_text'][0]])","metadata":{"execution":{"iopub.status.busy":"2022-09-17T12:42:13.437832Z","iopub.execute_input":"2022-09-17T12:42:13.438164Z","iopub.status.idle":"2022-09-17T12:42:13.446699Z","shell.execute_reply.started":"2022-09-17T12:42:13.438134Z","shell.execute_reply":"2022-09-17T12:42:13.445463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, it has converted lot of words into something meaningless. This might definitely reduce the size of the vocabulary at the end but we're losing information this way. So, it's not recommended.","metadata":{}},{"cell_type":"markdown","source":"## Lemmatization\n\nYou could say this is an improved version of stemming... without losing the meaning of the underlying words.","metadata":{}},{"cell_type":"code","source":"# Need to download this for the lemmatizer used below\nnltk.download('omw-1.4')","metadata":{"execution":{"iopub.status.busy":"2022-09-18T20:12:34.915973Z","iopub.execute_input":"2022-09-18T20:12:34.916825Z","iopub.status.idle":"2022-09-18T20:12:35.319764Z","shell.execute_reply.started":"2022-09-18T20:12:34.916784Z","shell.execute_reply":"2022-09-18T20:12:35.318526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import the lemmatizer\nfrom nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\n\n# Apply for the first document\nprint([lemmatizer.lemmatize(x) for x in df_input['tokenized_text'][0]])","metadata":{"execution":{"iopub.status.busy":"2022-09-18T20:12:47.259571Z","iopub.execute_input":"2022-09-18T20:12:47.259995Z","iopub.status.idle":"2022-09-18T20:12:49.571345Z","shell.execute_reply.started":"2022-09-18T20:12:47.259958Z","shell.execute_reply":"2022-09-18T20:12:49.569652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now, apply for all documents\ndf_input['tokenized_text'] = df_input['tokenized_text'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\ndf_input['tokenized_text'].head()","metadata":{"execution":{"iopub.status.busy":"2022-09-18T20:12:50.794487Z","iopub.execute_input":"2022-09-18T20:12:50.795435Z","iopub.status.idle":"2022-09-18T20:13:10.887842Z","shell.execute_reply.started":"2022-09-18T20:12:50.795393Z","shell.execute_reply":"2022-09-18T20:13:10.886475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelling\n\n<span id='mdl'/>\n\nUsing one of the simplest feature extraction technique in text analytics - **TfIdfVectorizer** which computes something called Term Frequency and Inverse Document Frequency to properly represent all the text documents in the corpus.\n\nOther ways one could do modelling is to use deep learning based models like LSTM/RNNs or Transformer based models like BERT to encode the input text into a latent feature space (embeddings) and then use these as features to classify into the target labels.","metadata":{}},{"cell_type":"code","source":"# Use Tf-Idf vectorizer on the already preprocessed, tokenized text\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ndef dummy_fun(doc):\n    return doc\n\nvectorizer = TfidfVectorizer(\n    analyzer='word',\n    tokenizer=dummy_fun, # This can't be None, hence using a dummy function\n    preprocessor=dummy_fun,\n    token_pattern=None)\n\nX = vectorizer.fit_transform(df_input['tokenized_text'].values)\nX.shape","metadata":{"execution":{"iopub.status.busy":"2022-09-18T20:13:10.891006Z","iopub.execute_input":"2022-09-18T20:13:10.892100Z","iopub.status.idle":"2022-09-18T20:13:13.535527Z","shell.execute_reply.started":"2022-09-18T20:13:10.892049Z","shell.execute_reply":"2022-09-18T20:13:13.533882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Encode the target labels into (0, 1, 2...) and split into train/test datasets.\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\ny = LabelEncoder().fit_transform(df_input['Target'].values)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1\n                                                    , random_state=42)\nprint('Shapes of X_train, X_test, y_train, y_test', X_train.shape, X_test.shape, y_train.shape, y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-09-18T20:13:13.538345Z","iopub.execute_input":"2022-09-18T20:13:13.538874Z","iopub.status.idle":"2022-09-18T20:13:13.579260Z","shell.execute_reply.started":"2022-09-18T20:13:13.538825Z","shell.execute_reply":"2022-09-18T20:13:13.577842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check what the vocabulary contains\nvectorizer.get_feature_names_out()","metadata":{"execution":{"iopub.status.busy":"2022-09-17T12:43:01.508745Z","iopub.execute_input":"2022-09-17T12:43:01.509882Z","iopub.status.idle":"2022-09-17T12:43:01.656192Z","shell.execute_reply.started":"2022-09-17T12:43:01.509823Z","shell.execute_reply":"2022-09-17T12:43:01.655152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define a common evaluation function that gives classification report and cross-validation score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report\n\ndef evaluate_model(sk_model, X_train_in, y_train_in, folds=5):\n    sk_model.fit(X_train, y_train)\n    y_pred = sk_model.predict(X_test)\n    print(classification_report(y_test, y_pred))\n    cv_score = cross_val_score(sk_model, X_train_in, y_train_in, cv=5)\n    print('Model:', sk_model.__class__.__name__)\n    print('{}-fold mean CV score:'.format(folds), round(cv_score.mean(), 4))","metadata":{"execution":{"iopub.status.busy":"2022-09-17T12:43:34.653659Z","iopub.execute_input":"2022-09-17T12:43:34.654527Z","iopub.status.idle":"2022-09-17T12:43:34.661435Z","shell.execute_reply.started":"2022-09-17T12:43:34.654481Z","shell.execute_reply":"2022-09-17T12:43:34.660275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Naive-Bayes as the baseline model\nfrom sklearn.naive_bayes import MultinomialNB\n\nevaluate_model(MultinomialNB(), X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 'hinge' loss implies the classifier is Linear SVM\nfrom sklearn.linear_model import SGDClassifier\n\nclf = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, max_iter=10, random_state=42)\nevaluate_model(clf, X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-09-17T12:43:50.303054Z","iopub.execute_input":"2022-09-17T12:43:50.303499Z","iopub.status.idle":"2022-09-17T12:43:51.100399Z","shell.execute_reply.started":"2022-09-17T12:43:50.303459Z","shell.execute_reply":"2022-09-17T12:43:51.099327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Multinomial logistic regression model\nfrom sklearn.linear_model import LogisticRegression \n\nclf = LogisticRegression(multi_class='multinomial', max_iter=200)\nevaluate_model(clf, X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparameter optimization\n\nTry optimizing the hyperparameters to get more accuracy hopefully without overfitting. GridSearchCV from sklearn package can be used.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nhyper_parameters = {'loss': ['hinge', 'modified_huber'], 'alpha': [1e-3, 0.01, 0.1], \n                    'eta0': [0.01, 0.05, 0.1],\n                    'max_iter': [10, 50, 100]}\n\nclf = GridSearchCV(SGDClassifier(), hyper_parameters)\nclf.fit(X_train, y_train)\nprint(clf.best_params_)\nprint(clf.best_score_)","metadata":{"execution":{"iopub.status.busy":"2022-09-18T20:14:20.959858Z","iopub.execute_input":"2022-09-18T20:14:20.960326Z","iopub.status.idle":"2022-09-18T20:19:01.563983Z","shell.execute_reply.started":"2022-09-18T20:14:20.960287Z","shell.execute_reply":"2022-09-18T20:19:01.562604Z"},"trusted":true},"execution_count":null,"outputs":[]}]}