{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, Dropout, SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 1s 0us/step\n",
      "17473536/17464789 [==============================] - 1s 0us/step\n",
      "Loaded dataset with 25000 training samples, 25000 test samples\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "vocabulary_size = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocabulary_size)\n",
    "print('Loaded dataset with {} training samples, {} test samples'.format(len(X_train), len(X_test)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: the as you with out themselves powerful lets loves their becomes reaching had journalist of lot from anyone to have after out atmosphere never more room and it so heart shows to years of every never going and help moments or of every chest visual movie except her was several of enough more with is now current film as you of mine potentially unfortunately of you than him that with out themselves her get for was camp of you movie sometimes movie that with scary but and to story wonderful that in seeing in character to of 70s and with heart had shadows they of here that with her serious to have does when from why what have critics they is you that isn't one will very to as itself with other and in of seen over and for anyone of and br show's to whether from than out themselves history he name half some br of and odd was two most of mean for 1 any an boat she he should is thought and but of script you not while history he heart to real at and but when from one bit then have two of script their with her nobody most that with wasn't to with armed acting watch an for with and film want an\n",
      "\n",
      "Label: 1\n",
      "Maximum review length: 2697\n",
      "Minimum review length: 70\n"
     ]
    }
   ],
   "source": [
    "# Get word index\n",
    "word_index = imdb.get_word_index()\n",
    "index_to_word = {i: word for word, i in word_index.items()}\n",
    "\n",
    "# Check a sample of the data\n",
    "print('Review:', ' '.join([index_to_word.get(i) for i in X_train[0]]))\n",
    "print('\\nLabel:', y_train[0])  # 1 for positive, 0 for negative\n",
    "\n",
    "# Check the maximum and minimum length of the reviews\n",
    "max_review_length = max([len(x) for x in X_train + X_test])\n",
    "min_review_length = min([len(x) for x in X_train + X_test])\n",
    "print('\\nMaximum review length:', max_review_length)\n",
    "print('Minimum review length:', min_review_length)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Pad the reviews to the same length\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen = max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen = max_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 500, 32)           160000    \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 100)               13300     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 173,401\n",
      "Trainable params: 173,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "embedding_size = 32  # Dimension of the embedding vector\n",
    "model = Sequential()\n",
    "# The input to the embedding layer is a matrix of integers (the indices of the words from word_index dict).\n",
    "# So, for each review, we have a matrix of shape (max_words, 1). As of now, each word is just a single integer.\n",
    "# The embedding layer will learn a vector of shape (max_words, embedding_size)\n",
    "model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
    "model.add(SimpleRNN(100))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "313/313 [==============================] - 151s 472ms/step - loss: 0.6719 - accuracy: 0.5751 - val_loss: 0.6319 - val_accuracy: 0.6548\n",
      "Epoch 2/3\n",
      "313/313 [==============================] - 150s 478ms/step - loss: 0.5907 - accuracy: 0.6799 - val_loss: 0.5359 - val_accuracy: 0.7352\n",
      "Epoch 3/3\n",
      "313/313 [==============================] - 162s 518ms/step - loss: 0.4927 - accuracy: 0.7653 - val_loss: 0.5857 - val_accuracy: 0.6944\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x2a454a835b0>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "batch_size = 64\n",
    "epochs = 3\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs = epochs, validation_split = 0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 51s 65ms/step - loss: 0.5727 - accuracy: 0.6984\n",
      "Test accuracy: 0.6984000205993652\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "score, acc = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', acc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7816201]]\n",
      "[[0.84944516]]\n"
     ]
    }
   ],
   "source": [
    "# Should print a very high score like 0.98.\n",
    "test_1 = np.array([word_index[j] for j in \"i loved it highly recommend it to anyone and everyone looking for a great movie to watch\".split()])\n",
    "test_1 = sequence.pad_sequences([test_1], maxlen = max_words)\n",
    "print(model.predict(test_1))\n",
    "\n",
    "# Should print a very low score like 0.01.\n",
    "test_2 = np.array([ word_index[j] for j in \"this was awful i hated it so much nobody should watch this the acting was terrible the music was terrible overall it was just bad\".split()])\n",
    "test_2 = sequence.pad_sequences([test_2], maxlen = max_words)\n",
    "print(model.predict(test_2))  # I guess, it misclassified the review as positive."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Further improvements:\n",
    "# - Try to use a bidirectional RNN.\n",
    "# - Try Dropout.\n",
    "# - Try LSTM.\n",
    "# - Maybe include more stuff in the vocabulary, like punctuation, etc."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}