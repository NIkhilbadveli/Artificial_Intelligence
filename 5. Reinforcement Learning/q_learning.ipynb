{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### We will try to solve the basic problem of CartPole balancing using simple Q-learning.\n",
    "[Article link](https://medium.com/analytics-vidhya/q-learning-is-the-most-basic-form-of-reinforcement-learning-which-doesnt-take-advantage-of-any-8944e02570c5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "state_space = 4  # number of state variables\n",
    "action_space = 2  # number of actions\n",
    "\n",
    "\n",
    "def Qtable(state_space, action_space, bin_size=30):\n",
    "    bins = [np.linspace(-4.8, 4.8, bin_size),  # cart position\n",
    "            np.linspace(-4, 4, bin_size),  # cart velocity\n",
    "            np.linspace(-0.418, 0.418, bin_size),  # pole angle (radians)\n",
    "            np.linspace(-4, 4, bin_size)]  # pole angular velocity\n",
    "\n",
    "    q_table = np.random.uniform(low=-1, high=1, size=([bin_size] * state_space + [action_space]))\n",
    "    return q_table, bins\n",
    "\n",
    "\n",
    "def Discrete(state, bins):\n",
    "    index = []\n",
    "    for i in range(len(state)): index.append(np.digitize(state[i], bins[i]) - 1)\n",
    "    return tuple(index)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def Q_learning(q_table, bins, episodes=5000, gamma=0.95, lr=0.1, timestep=100, epsilon=0.2):\n",
    "    total_score = 0\n",
    "    steps = 0\n",
    "    curr_score_history = []\n",
    "    for episode_i in range(1, episodes + 1):\n",
    "        steps += 1\n",
    "        # env.reset() => initial observation\n",
    "        current_state = Discrete(env.reset(), bins)\n",
    "\n",
    "        current_score = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            if episode_i % timestep == 0: env.render()\n",
    "            if np.random.uniform(0, 1) < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = np.argmax(q_table[current_state])\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            next_state = Discrete(observation, bins)\n",
    "            current_score += reward  # Accumulate the current_score until done\n",
    "\n",
    "            if not done:\n",
    "                max_future_q = np.max(q_table[next_state])\n",
    "                current_q = q_table[current_state + (action,)]\n",
    "                new_q = (1 - lr) * current_q + lr * (reward + gamma * max_future_q)\n",
    "                q_table[current_state + (action,)] = new_q\n",
    "            else:\n",
    "                total_score += current_score\n",
    "            current_state = next_state\n",
    "        curr_score_history.append(current_score)\n",
    "        avg_score = total_score / episode_i\n",
    "        if episode_i % timestep == 0: print('Average score after {} episodes:- {}'.format(episode_i, avg_score))\n",
    "        if avg_score >= 150 or current_score >= 150:  # This success score is completely upto us.\n",
    "            print('Problem solved in episode {} with steps {}'.format(episode_i, steps))\n",
    "            return curr_score_history\n",
    "    return curr_score_history"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Looks like using current score as stopping criterion is not enough, since there's wild fluctuations in the performance of the agent. It is indeed better to use Average score to achieve the desired score, although this doesn't guarantee anything if run again with the same Q-table."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "q_table, bins = Qtable(4, 2)\n",
    "score_history = Q_learning(q_table, bins, episodes=10000, gamma=0.995, lr=0.15)\n",
    "env.close()\n",
    "# plt.plot(score_history)\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}